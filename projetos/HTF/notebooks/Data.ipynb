{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização de Dados\n",
    "\n",
    "Notebook destinado à visualização de dados brutos dos datasets. Os datasets escolhidos para treinamento e validação do primeiro modelo da S-GAN são:\n",
    "\n",
    "* ~~ETH Pedestrian~~ (Repository Offline);\n",
    "* [UCY Crowds Data](https://graphics.cs.ucy.ac.cy/portfolio);\n",
    "* ~~[Stanford Drone Dataset (SDD) - Original Dataset](https://cvgl.stanford.edu/projects/uav_data/)~~ (Offline);\n",
    "* [Stanford Drone Dataset (SDD) - Kaggle Compressed](https://www.kaggle.com/datasets/aryashah2k/stanford-drone-dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rarfile # .rar extensions manipulation\n",
    "#!apt-get update\n",
    "#!apt-get install -y unrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import socket\n",
    "import rarfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Global vars:\n",
    "colab_usage = True if 'google.colab' in socket.gethostname() else False # Google Colab usage\n",
    "\n",
    "# Dataset Useful Links:\n",
    "eth_ucy_dataset = \"https://www.dropbox.com/s/8n02xqv3l9q18r1/datasets.zip?dl=0\"\n",
    "ucy_dataset = \"https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data.zip\"\n",
    "sdd_campus_dataset = \"https://storage.googleapis.com/kaggle-data-sets/1433833/2373439/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20241005%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20241005T231629Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=338abfd23b3e4bf78ba1c1c6656aee077d9d817963bdfe6af1f917922af58e96183922001c13d5049305aa2295e5577a1bc85adfb9ae44433831c360c017b06ca77ac7244e6540bf9c0e606eda2252df8ef85e62dc96ef1b721f912e5b6d54feb76a8b9e599b269593ee8ec855e4840a6f5263430745d314bc39b880ba65aca0f9601d3bb736d305eebc99b6334bf91546d27df8cf8c7dc33e4aaa129dd8288aef8df6574e979e9bad1e1e585ae1514e247498c805cc91fa6c75ed6a9e5e2124220218352025179e2ad31552f24e39dee8122149bf2efc8b257e041506888655fdeb702ff52bfbc14e60adddbc7864f62872c201701353106c218227e82d89f5\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data path prefix\n",
    "data_path_prefix = \"../content/\" if colab_usage else \"../\"\n",
    "\n",
    "# Create the directory if it doesn't exist in Google Colab\n",
    "if colab_usage:\n",
    "    raw_data_path = f\"{data_path_prefix}data/raw\"\n",
    "    os.makedirs(raw_data_path, exist_ok=True)\n",
    "\n",
    "# Define paths for UCY and SDD datasets\n",
    "eth_ucy_data_path = f\"{data_path_prefix}data/raw/ETH-UCY-AO\"\n",
    "ucy_data_path     = f\"{data_path_prefix}data/raw/UCY\"\n",
    "sdd_data_path     = f\"{data_path_prefix}data/raw/SDD\"\n",
    "\n",
    "# Create UCY & SDD directory\n",
    "os.makedirs(eth_ucy_data_path, exist_ok=True) # creating ETH-UCY-AO raw data path\n",
    "os.makedirs(ucy_data_path, exist_ok=True)     # creating UCY raw data path\n",
    "os.makedirs(sdd_data_path, exist_ok=True)     # creating SDD raw data path\n",
    "    \n",
    "# Create a temporary directory:\n",
    "os.makedirs(\"./tmp\", exist_ok=True) # creating temporary path\n",
    "\n",
    "# Print paths for verification\n",
    "print(f\"ETH-UCY-AO data path: {eth_ucy_data_path} {'created' if os.path.isdir(eth_ucy_data_path) else 'not created'}.\")\n",
    "print(f\"UCY data path: {ucy_data_path} {'created' if os.path.isdir(ucy_data_path) else 'not created'}.\")\n",
    "print(f\"SDD data path: {sdd_data_path} {'created' if os.path.isdir(sdd_data_path) else 'not created'}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ETH-UCY Dataset (annotations only):\n",
    "# download\n",
    "if (not os.path.exists(\"./tmp/eth-ucy-crowds-data.zip\")):\n",
    "    subprocess.run([\"wget\", \"-O\", \"./tmp/eth-ucy-crowds-data.zip\", eth_ucy_dataset])\n",
    "\n",
    "# extract dataset\n",
    "if (os.path.exists(\"./tmp/eth-ucy-crowds-data.zip\")):\n",
    "    subprocess.run([\"unzip\", \"-q\", \"./tmp/eth-ucy-crowds-data.zip\", \"-d\", \"./tmp/\"])\n",
    "    subprocess.run([\"mv\", \"./tmp/datasets\", \"./tmp/ETH-UCY-AO\"]) # Annotation Only (AO)\n",
    "    shutil.copytree('./tmp/ETH-UCY-AO', eth_ucy_data_path, dirs_exist_ok=True)\n",
    "\n",
    "# clean ETH-UCY-AO path\n",
    "subprocess.run([\"rm\", \"-rf\", \"./tmp/ETH-UCY-AO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare UCY Dataset:\n",
    "# list of interesting files\n",
    "ucy_list = [\"crowds_zara01\",\"crowds_zara02\",\"students003\"] # only data + video list\n",
    "ucy_list_rar = [\"data_zara.rar\", \"data_university_students.rar\"]\n",
    "ucy_list_ext_copy = [\".vsp\", \".avi\"]\n",
    "\n",
    "# download\n",
    "if (not os.path.exists(\"./tmp/crowd-data.zip\")):\n",
    "    subprocess.run([\"wget\", \"-O\", \"./tmp/crowd-data.zip\", ucy_dataset, \"--no-check-certificate\"]) # warning for --no-check-certificate usage\n",
    "\n",
    "# unzip main compressed file\n",
    "if (not os.path.isdir(\"./tmp/crowds\")):\n",
    "    subprocess.run([\"unzip\", \"-q\", \"./tmp/crowd-data.zip\", \"-d\", \"./tmp\"])\n",
    "\n",
    "# extract datasets\n",
    "for ucy_rf in ucy_list_rar:\n",
    "    with rarfile.RarFile(f'./tmp/crowds/data/{ucy_rf}') as rf:\n",
    "        rf.extractall('./tmp/crowds/data/') # extract .rar in the same directory\n",
    "        print(f\"Extracted './tmp/crowds/data/{ucy_rf}'\")\n",
    "\n",
    "# copy to raw data path\n",
    "for ucy_data in ucy_list:\n",
    "    for ext in ucy_list_ext_copy:\n",
    "        source_path = f\"./tmp/crowds/data/{ucy_data}{ext}\"\n",
    "        destination_path = ucy_data_path\n",
    "\n",
    "        # Move the file from source to destination\n",
    "        if (not os.path.exists(f'{destination_path}/{ucy_data}{ext}')):\n",
    "            shutil.move(source_path, destination_path)\n",
    "            print(f\"Moved {source_path} to {destination_path}\")\n",
    "        else:\n",
    "            print(f\"{destination_path}/{ucy_data}{ext} already exists.\")\n",
    "\n",
    "# clean crowds path\n",
    "subprocess.run([\"rm\", \"-rf\", \"./tmp/crowds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Stanford Drone Dataset:\n",
    "# list of interesting files\n",
    "sdd_path_dirs = [\"video\", \"annotations\"] # video & annotations path - each internal folder has its own correspondent in the other path\n",
    "\n",
    "# download\n",
    "if (not os.path.exists(\"./tmp/sdd-data.zip\")):\n",
    "    subprocess.run([\"wget\", \"-O\", \"./tmp/sdd-data.zip\", sdd_campus_dataset])\n",
    "\n",
    "# create auxiliary subdirectory\n",
    "os.makedirs(\"./tmp/stanford\", exist_ok=True)\n",
    "\n",
    "# unzip main compressed file\n",
    "if (\n",
    "    not os.path.isdir(\"./tmp/stanford/annotations\") and # to consider the two subfolders (correct structure of the dataset)\n",
    "    not os.path.isdir(\"./tmp/stanford/video\")\n",
    "):\n",
    "    # unzipping the main compressed file\n",
    "    subprocess.run([\"unzip\", \"-q\", \"./tmp/sdd-data.zip\", \"-d\", \"./tmp/stanford\"])\n",
    "\n",
    "# it's not needed to extract internal datasets - it has its own folder structure\n",
    "\n",
    "# copy to raw data path\n",
    "for primary_folder in os.listdir(\"./tmp/stanford/\"):\n",
    "    ''' \n",
    "    Since there are always two folders, we can process in one loop together. Additionally, \n",
    "    because these folders are mirrored, we only need to copy their internal contents\n",
    "    to the reference raw data path, simplifying the process.\n",
    "    '''\n",
    "    for secondary_folder in os.listdir(\"./tmp/stanford/\" + primary_folder):\n",
    "        relative_path = f\"./tmp/stanford/{primary_folder}/{secondary_folder}\"\n",
    "\n",
    "        print(f\"Processing: {relative_path} \", end='')\n",
    "\n",
    "        # Find and remove .jpeg and .jpg files in the secondary folders (unuseful files)\n",
    "        subprocess.run([\"find\", relative_path, \"-type\", \"f\", \"-name\", \"*.jpeg\", \"-or\", \"-name\", \"*.jpg\", \"-delete\"])\n",
    "\n",
    "        # Copy all the subfolders to the reference raw data path - devoted to SDD dataset\n",
    "        if (os.path.isdir(relative_path)): # to avoid bugs\n",
    "            shutil.copytree(relative_path, f\"{sdd_data_path}/{secondary_folder}\", dirs_exist_ok=True)\n",
    "            print(f\"copied to {sdd_data_path}/{secondary_folder}\")\n",
    "\n",
    "\n",
    "# clean stanford dataset path\n",
    "subprocess.run([\"rm\", \"-rf\", \"./tmp/stanford\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETH-UCY Crowds Dataset (Annotations Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    '''\n",
    "    Args:\n",
    "    - data_dir: Directory containing dataset files in the format\n",
    "    <frame_id> <ped_id> <x> <y>\n",
    "    - obs_len: Number of time-steps in input trajectories\n",
    "    - pred_len: Number of time-steps in output trajectories\n",
    "    - skip: Number of frames to skip while making the dataset\n",
    "    - threshold: Minimum error to be considered for non linear traj\n",
    "    when using a linear predictor\n",
    "    - min_ped: Minimum number of pedestrians that should be in a seqeunce\n",
    "    - delim: Delimiter in the dataset files\n",
    "\n",
    "    The structure is: <frame_id> <pedestrian_id> <x> <y>\n",
    "    '''\n",
    "    delim = '\\t'\n",
    "\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(\n",
    "        ETHUCY_data_dir['eth']['train'] + '/biwi_hotel_train.txt',\n",
    "        delimiter=delim, header=None, names=['frame_id', 'ped_id', 'x', 'y']\n",
    "    )\n",
    "\n",
    "    # Convert frame_id and ped_id to integers\n",
    "    df['frame_id'] = df['frame_id'].astype(int)\n",
    "    df['ped_id'] = df['ped_id'].astype(int)\n",
    "\n",
    "    # Group data by frame_id and aggregate the pedestrian data\n",
    "    grouped_data = df.groupby('frame_id').agg({\n",
    "        'ped_id' : list, # Collect all pedestrian IDs for the frame\n",
    "        'x'      : list, # Collect all respective x positions for the frame\n",
    "        'y'      : list, # Collect all respective y positions for the frame\n",
    "    }).reset_index()\n",
    "\n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the datasets in a dictionary structure\n",
    "ETHUCY_datasets = [\"eth\", \"hotel\", \"raw\", \"univ\", \"zara1\", \"zara2\"]\n",
    "ETHUCY_data_dir = {\n",
    "    dataset: {\n",
    "        'test' : f'{eth_ucy_data_path}/{dataset}/test',\n",
    "        'train' : f'{eth_ucy_data_path}/{dataset}/train',\n",
    "        'val' : f'{eth_ucy_data_path}/{dataset}/val'\n",
    "    } for dataset in ETHUCY_datasets\n",
    "}\n",
    "\n",
    "# Check for existing datasets in the ETH-UCY-AO directory\n",
    "for dataset in ETHUCY_datasets:\n",
    "    print(dataset)\n",
    "\n",
    "# Reading & plotting the dataset\n",
    "'''\n",
    "Example of reading the dataset\n",
    "j = 0\n",
    "frame_step = 10 # frames jump 10 by 10\n",
    "frame_id   = 10 * j\n",
    "ped_at_frame = grouped_data[grouped_data['frame_id'] == frame_id]\n",
    "\n",
    "ped_id = 1\n",
    "ped_at_index = ped_at_frame['ped_id'].iloc[0].index(ped_id) # ped_id starts from 1.0\n",
    "x_pos = ped_at_frame['x'].iloc[0][ped_at_index]\n",
    "y_pos = ped_at_frame['y'].iloc[0][ped_at_index]\n",
    "print(f'{ped_id}: ({x_pos}, {y_pos})')\n",
    "'''\n",
    "\n",
    "'''\n",
    "TODO: colocar a velocidade no dataframe\n",
    "TODO: considerar vetor de interação entre os pedestres vetor e.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCY Crowds Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Drone Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always must clean the temporary directory to avoid github issues (.git)\n",
    "subprocess.run([\"rm\", \"-rf\", \"./tmp\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
